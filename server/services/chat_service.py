# services/chat_service.py

# Import necessary modules
import asyncio
import json
import time
from typing import Dict, Any, List, Optional

# Import service modules
from models.tool_model import ToolInfoJson
from services.db_service import db_service
from services.db_optimization_service import get_db_optimization_service
from services.config_service import USER_DATA_DIR, DEFAULT_PROVIDERS_CONFIG
from services.langgraph_service import langgraph_multi_agent
from services.websocket_service import send_to_websocket, send_ai_thinking_status
from services.stream_service import add_stream_task, remove_stream_task
from models.config_model import ModelInfo
from log import get_logger
import os

logger = get_logger(__name__)

# è·å–ä¼˜åŒ–çš„æ•°æ®åº“æœåŠ¡å®ä¾‹
DB_PATH = os.path.join(USER_DATA_DIR, "localmanus.db")
db_opt_service = get_db_optimization_service(DB_PATH)


def find_model_config(provider: str, model_name: str) -> ModelInfo:
    """
    æ ¹æ® provider å’Œ model åç§°ä» DEFAULT_PROVIDERS_CONFIG ä¸­æŸ¥æ‰¾å®Œæ•´çš„æ¨¡å‹é…ç½®
    
    Args:
        provider: æ¨¡å‹æä¾›å•† (å¦‚ 'google', 'openai')
        model_name: æ¨¡å‹åç§° (å¦‚ 'gemini-2.5-flash-image')
        
    Returns:
        å®Œæ•´çš„ ModelInfo é…ç½®
    """
    
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    if provider in DEFAULT_PROVIDERS_CONFIG:
        provider_config = DEFAULT_PROVIDERS_CONFIG[provider]
        models = provider_config.get('models', {})
        if model_name in models:
            return {
                'provider': provider,
                'model': model_name,
                'url': provider_config.get('url', ''),
                'type': 'text'  # å¼ºåˆ¶è®¾ç½®ä¸ºæ–‡æœ¬ç±»å‹
            }
            
    # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
    for config_provider, provider_config in DEFAULT_PROVIDERS_CONFIG.items():
        models = provider_config.get('models', {})
        for config_model in models.keys():
            # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åŒ…å«å…³é”®è¯
            if (provider.lower() in config_provider.lower() or 
                config_provider.lower() in provider.lower() or
                model_name.lower() in config_model.lower() or
                config_model.lower() in model_name.lower()):
                
                logger.info(f"[debug] æ¨¡ç³ŠåŒ¹é…æˆåŠŸ: {provider}/{model_name} -> {config_provider}/{config_model}")
                return {
                    'provider': config_provider,
                    'model': config_model,
                    'url': provider_config.get('url', ''),
                    'type': 'text'
                }
    
    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    logger.warning(f"[warning] æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡å‹é…ç½®: {provider}/{model_name}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # å¦‚æœæä¾›å•†å­˜åœ¨ï¼Œä½¿ç”¨è¯¥æä¾›å•†çš„ç¬¬ä¸€ä¸ªæ–‡æœ¬æ¨¡å‹
    if provider in DEFAULT_PROVIDERS_CONFIG:
        provider_config = DEFAULT_PROVIDERS_CONFIG[provider]
        models = provider_config.get('models', {})
        text_models = {k: v for k, v in models.items() if v.get('type') == 'text'}
        if text_models:
            first_model = next(iter(text_models.keys()))
            return {
                'provider': provider,
                'model': first_model,
                'url': provider_config.get('url', ''),
                'type': 'text'
            }
    
    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ OpenAI
    openai_config = DEFAULT_PROVIDERS_CONFIG.get('openai', {})
    openai_models = openai_config.get('models', {})
    first_openai_model = next(iter(openai_models.keys())) if openai_models else 'gpt-4o-mini'
    
    return {
        'provider': 'openai',
        'model': first_openai_model,
        'url': openai_config.get('url', 'https://api.openai.com/v1'),
        'type': 'text'
    }


async def handle_chat(data: Dict[str, Any]) -> None:
    """
    Handle an incoming chat request.

    Workflow:
    - Parse incoming chat data.
    - Optionally inject system prompt.
    - Save chat session and messages to the database.
    - Launch langgraph_agent task to process chat.
    - Manage stream task lifecycle (add, remove).
    - Notify frontend via WebSocket when stream is done.

    Args:
        data (dict): Chat request data containing:
            - messages: list of message dicts
            - session_id: unique session identifier
            - canvas_id: canvas identifier (contextual use)
            - text_model: text model configuration
            - tool_list: list of tool model configurations (images/videos)
            - user_info: user information (optional)
    """
    start_time = time.time()
    logger.info(f"[debug] === å¼€å§‹å¤„ç†èŠå¤©è¯·æ±‚ ===")
    
    # Extract fields from incoming data
    messages: List[Dict[str, Any]] = data.get('messages', [])
    session_id: str = data.get('session_id', '')
    canvas_id: str = data.get('canvas_id', '')
    text_model_raw = data.get('text_model', None)
    tool_list: List[ToolInfoJson] = data.get('tool_list', [])
    template_id: str = data.get('template_id', '')
    user_info: Dict[str, Any] = data.get('user_info', {})
    
    # æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼šå¦‚æœæ²¡æœ‰æ–‡æœ¬æ¨¡å‹ï¼Œå°†ç¬¬ä¸€ä¸ªå·¥å…·æ¨¡å‹è½¬æ¢ä¸ºæ–‡æœ¬æ¨¡å‹
    text_model: ModelInfo
    if text_model_raw and isinstance(text_model_raw, dict) and 'provider' in text_model_raw and 'model' in text_model_raw:
        # æœ‰æ–‡æœ¬æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨
        text_model = text_model_raw
        logger.info(f"[debug] ä½¿ç”¨æ–‡æœ¬æ¨¡å‹: {text_model.get('provider', '')}/{text_model.get('model', '')}")
    elif tool_list and len(tool_list) > 0:
        # æ²¡æœ‰æ–‡æœ¬æ¨¡å‹ä½†æœ‰å·¥å…·æ¨¡å‹ï¼Œå°†ç¬¬ä¸€ä¸ªå·¥å…·æ¨¡å‹è½¬æ¢ä¸ºæ–‡æœ¬æ¨¡å‹
        first_tool = tool_list[0]
        provider = first_tool.get('provider', '')
        model_name = first_tool.get('display_name') or first_tool.get('id', '')
        
        # ä½¿ç”¨æ™ºèƒ½é…ç½®åŒ¹é…è·å–å®Œæ•´é…ç½®
        text_model = dict(find_model_config(provider, model_name))
        logger.info(f"[debug] å°†å·¥å…·æ¨¡å‹è½¬æ¢ä¸ºæ–‡æœ¬æ¨¡å‹: {provider}/{model_name} -> {text_model.get('provider', '')}/{text_model.get('model', '')} (URL: {text_model.get('url', '')})")
    else:
        # æ—¢æ²¡æœ‰æ–‡æœ¬æ¨¡å‹ä¹Ÿæ²¡æœ‰å·¥å…·æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        logger.warning("[warning] æ—¢æ²¡æœ‰æ–‡æœ¬æ¨¡å‹ä¹Ÿæ²¡æœ‰å·¥å…·æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤ OpenAI é…ç½®")
        text_model = dict(find_model_config('openai', 'gpt-4o-mini'))
    
    # Validate required fields
    if not session_id or session_id.strip() == '':
        logger.error("[error] session_id is required but missing or empty")
        raise ValueError("session_id is required")
    
    # Extract user information
    user_uuid = user_info.get('uuid') if user_info else None
    
    logger.info(f"[debug] è¯·æ±‚å‚æ•°: session_id={session_id}, canvas_id={canvas_id}, user_uuid={user_uuid}")
    logger.info(f"[debug] æ¶ˆæ¯æ•°é‡: {len(messages)}, å·¥å…·æ•°é‡: {len(tool_list)}")
    logger.info(f"[debug] æœ€ç»ˆä½¿ç”¨çš„æ–‡æœ¬æ¨¡å‹: {text_model.get('provider', '')}/{text_model.get('model', '')}")

    # If template_id is provided, get template prompt
    template_start = time.time()
    template_prompt: Optional[str] = None
    if template_id:
        from routers.templates_router import TEMPLATES
        template = next((t for t in TEMPLATES if t["id"] == int(template_id)), None)
        if template:
            template_prompt = template.get("prompt")
            logger.info(f"[debug] æ¨¡æ¿åŠ è½½è€—æ—¶: {(time.time() - template_start) * 1000:.2f}ms")

    # TODO: save and fetch system prompt from db or settings config
    system_prompt: Optional[str] = data.get('system_prompt')

    # Database operations - ä¼˜åŒ–ä¸ºæ‰¹é‡æ“ä½œ
    db_start = time.time()
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦æ‰§è¡Œçš„æ•°æ®åº“æ“ä½œ
    db_operations = []
    
    # If there is only one message, create a new chat session
    if len(messages) == 1:
        # create new session
        prompt = messages[0].get('content', '')
        title = prompt[:200] if isinstance(prompt, str) else ''
        # æ­£ç¡®ä¼ é€’å‚æ•°ï¼šid, model, provider, canvas_id, user_uuid, title
        await db_service.create_chat_session(session_id, text_model.get('model', ''), text_model.get('provider', ''), canvas_id, user_uuid, title)
        logger.info(f"[debug] åˆ›å»ºèŠå¤©ä¼šè¯: session_id={session_id}, user_uuid={user_uuid}")

    # æ‰¹é‡åˆ›å»ºæ¶ˆæ¯
    if len(messages) > 0:
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨å•ä¸ªæ¶ˆæ¯åˆ›å»ºï¼Œä½†æ·»åŠ äº†æ€§èƒ½ç›‘æ§
        await db_service.create_message(session_id, messages[-1].get('role', 'user'), json.dumps(messages[-1]), user_uuid)
    
    logger.info(f"[debug] æ•°æ®åº“æ“ä½œè€—æ—¶: {(time.time() - db_start) * 1000:.2f}ms")
    
    # è·å–æ•°æ®åº“æ€§èƒ½ç»Ÿè®¡
    db_stats = await db_opt_service.get_stats()
    logger.info(f"[debug] æ•°æ®åº“ç»Ÿè®¡: {db_stats}")

    # ç«‹å³æ¨é€ç”¨æˆ·æ¶ˆæ¯åˆ°å‰ç«¯ï¼ˆç¡®ä¿ç”¨æˆ·çœ‹åˆ°è‡ªå·±çš„è¾“å…¥ï¼‰
    if len(messages) > 0:
        user_message = messages[-1]  # æœ€åä¸€æ¡æ¶ˆæ¯é€šå¸¸æ˜¯ç”¨æˆ·è¾“å…¥
        if user_message.get('role') == 'user':
            logger.info(f"[debug] ç«‹å³æ¨é€ç”¨æˆ·æ¶ˆæ¯åˆ°å‰ç«¯")
            await send_to_websocket(session_id, {
                'type': 'all_messages',
                'messages': messages  # å‘é€åŒ…å«ç”¨æˆ·æ¶ˆæ¯çš„å®Œæ•´åˆ—è¡¨
            })

            # å‘é€AIæ€è€ƒçŠ¶æ€ï¼ˆè®©ç”¨æˆ·çŸ¥é“AIæ­£åœ¨å¤„ç†ï¼‰
            logger.info(f"ğŸ§  [THINKING_DEBUG] å³å°†å‘é€AIæ€è€ƒçŠ¶æ€: session_id={session_id}, canvas_id={canvas_id}")
            await send_ai_thinking_status(session_id, canvas_id)
            logger.info(f"ğŸ§  [THINKING_DEBUG] AIæ€è€ƒçŠ¶æ€å‘é€è°ƒç”¨å®Œæˆ")

    # Create and start langgraph_agent task for chat processing
    task_start = time.time()
    task = asyncio.create_task(langgraph_multi_agent(
        messages, canvas_id, session_id, text_model, tool_list, system_prompt, template_id, template_prompt, user_uuid))
    logger.info(f"[debug] ä»»åŠ¡åˆ›å»ºè€—æ—¶: {(time.time() - task_start) * 1000:.2f}ms")

    # Register the task in stream_tasks (for possible cancellation)
    add_stream_task(session_id, task)
    logger.info(f"[debug] è¯·æ±‚é¢„å¤„ç†æ€»è€—æ—¶: {(time.time() - start_time) * 1000:.2f}ms")
    
    try:
        # Await completion of the langgraph_agent task
        agent_start = time.time()
        await task
        logger.info(f"[debug] Agentå¤„ç†è€—æ—¶: {(time.time() - agent_start) * 1000:.2f}ms")
    except asyncio.exceptions.CancelledError:
        logger.info(f"[debug] Session {session_id} cancelled during stream")
    finally:
        # Always remove the task from stream_tasks after completion/cancellation
        remove_stream_task(session_id)
        # Notify frontend WebSocket that chat processing is done
        await send_to_websocket(session_id, {
            'type': 'done'
        })
        logger.info(f"[debug] === èŠå¤©è¯·æ±‚å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {(time.time() - start_time) * 1000:.2f}ms ===")
